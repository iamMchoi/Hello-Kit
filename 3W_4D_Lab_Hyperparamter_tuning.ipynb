{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_Hyperparamter_tuning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamMchoi/Hello-Kit/blob/master/Lab_Hyperparamter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G1CLeL2q68gI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning"
      ]
    },
    {
      "metadata": {
        "id": "TW1mvJM17IE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf  # deep learning library. Tensors are just multi-dimensional arrays\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.backend as K\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYovY1PK7WAw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Import"
      ]
    },
    {
      "metadata": {
        "id": "YiaKyTYP7KUE",
        "colab_type": "code",
        "outputId": "79a4ce13-ba33-433d-ad37-5557d1bf4f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xzwvjuka7Ydx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNxXKoTh7aRi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compare Learning Rates"
      ]
    },
    {
      "metadata": {
        "id": "zpgWCeHE7cT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "dflist = []\n",
        "\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "  K.clear_session()\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
        "  model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
        "\n",
        "  model.compile(optimizer=SGD(lr=lr),  # Good default optimizer to start with\n",
        "                loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "                metrics=['accuracy'])  # what to track\n",
        "\n",
        "  h =  model.fit(x_train, y_train, batch_size=16, verbose=0)  \n",
        "\n",
        "  dflist.append(pd.DataFrame(h.history, index=h.epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0H51l_jm8uEX",
        "colab_type": "code",
        "outputId": "14859735-e691-406d-ec66-2081e93f2df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "h.history.keys()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "gf9ZdhFx7sJS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "historydf = pd.concat(dflist, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s93RIFcG7gdB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metrics_reported = dflist[0].columns\n",
        "idx = pd.MultiIndex.from_product([learning_rates, metrics_reported],\n",
        "                                 names=['learning_rate', 'metric'])\n",
        "\n",
        "historydf.columns = idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8LH8r7s8126",
        "colab_type": "code",
        "outputId": "a2382211-7d83-46ed-f82e-8c6b7f5d5e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "historydf"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>learning_rate</th>\n",
              "      <th colspan=\"2\" halign=\"left\">0.01</th>\n",
              "      <th colspan=\"2\" halign=\"left\">0.05</th>\n",
              "      <th colspan=\"2\" halign=\"left\">0.10</th>\n",
              "      <th colspan=\"2\" halign=\"left\">0.50</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.804367</td>\n",
              "      <td>0.767849</td>\n",
              "      <td>0.895783</td>\n",
              "      <td>0.361603</td>\n",
              "      <td>0.913883</td>\n",
              "      <td>0.289819</td>\n",
              "      <td>0.921633</td>\n",
              "      <td>0.256541</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "learning_rate      0.01                0.05                0.10            \\\n",
              "metric              acc      loss       acc      loss       acc      loss   \n",
              "0              0.804367  0.767849  0.895783  0.361603  0.913883  0.289819   \n",
              "\n",
              "learning_rate      0.50            \n",
              "metric              acc      loss  \n",
              "0              0.921633  0.256541  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "zdHAw21y7jFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Batch Size"
      ]
    },
    {
      "metadata": {
        "id": "F3Oqw_uu7luh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dflist = []\n",
        "\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "  K.clear_session()\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
        "  model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
        "\n",
        "  model.compile(optimizer='sgd',  # Good default optimizer to start with\n",
        "                loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "                metrics=['accuracy'])  # what to track\n",
        "\n",
        "  h =  model.fit(x_train, y_train, batch_size=batch_size, verbose=0)  \n",
        "\n",
        "  dflist.append(pd.DataFrame(h.history, index=h.epoch))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYng3HBE9aik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "historydf = pd.concat(dflist, axis=1)\n",
        "metrics_reported = dflist[0].columns\n",
        "idx = pd.MultiIndex.from_product([batch_sizes, metrics_reported],\n",
        "                                 names=['batch_size', 'metric'])\n",
        "historydf.columns = idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hhWMLcct9bid",
        "colab_type": "code",
        "outputId": "b1b7d939-96e0-40ba-b002-8fb1629cd465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "historydf"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>batch_size</th>\n",
              "      <th colspan=\"2\" halign=\"left\">16</th>\n",
              "      <th colspan=\"2\" halign=\"left\">32</th>\n",
              "      <th colspan=\"2\" halign=\"left\">64</th>\n",
              "      <th colspan=\"2\" halign=\"left\">128</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.81975</td>\n",
              "      <td>0.725545</td>\n",
              "      <td>0.74135</td>\n",
              "      <td>1.077969</td>\n",
              "      <td>0.64755</td>\n",
              "      <td>1.578685</td>\n",
              "      <td>0.502817</td>\n",
              "      <td>2.039209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "batch_size      16                 32                 64                  128  \\\n",
              "metric          acc      loss      acc      loss      acc      loss       acc   \n",
              "0           0.81975  0.725545  0.74135  1.077969  0.64755  1.578685  0.502817   \n",
              "\n",
              "batch_size            \n",
              "metric          loss  \n",
              "0           2.039209  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "PvtJZcqo9cRt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimizers"
      ]
    },
    {
      "metadata": {
        "id": "Re3tla6W9fQ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD, Adam, Adagrad, RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1iHWN0jS9hVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dflist = []\n",
        "\n",
        "optimizers = ['SGD(lr=0.01)',\n",
        "              'SGD(lr=0.01, momentum=0.3)',\n",
        "              'SGD(lr=0.01, momentum=0.3, nesterov=True)',  \n",
        "              'Adam(lr=0.01)',\n",
        "              'Adagrad(lr=0.01)',\n",
        "              'RMSprop(lr=0.01)']\n",
        "\n",
        "for opt_name in optimizers:\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
        "  model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
        "\n",
        "  model.compile(optimizer=eval(opt_name),  # Good default optimizer to start with\n",
        "                loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "                metrics=['accuracy'])  # what to track\n",
        "\n",
        "  h =  model.fit(x_train, y_train, batch_size=16, verbose=0, epochs=5)  \n",
        "\n",
        "  dflist.append(pd.DataFrame(h.history, index=h.epoch))     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1EUs4g8E9o_8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "historydf = pd.concat(dflist, axis=1)\n",
        "metrics_reported = dflist[0].columns\n",
        "idx = pd.MultiIndex.from_product([optimizers, metrics_reported],\n",
        "                                 names=['optimizers', 'metric'])\n",
        "historydf.columns = idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KVvXFudm9qca",
        "colab_type": "code",
        "outputId": "c20bd744-9230-482f-8ec8-fac6d9b69abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "historydf"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>optimizers</th>\n",
              "      <th colspan=\"2\" halign=\"left\">SGD(lr=0.01)</th>\n",
              "      <th colspan=\"2\" halign=\"left\">SGD(lr=0.01, momentum=0.3)</th>\n",
              "      <th colspan=\"2\" halign=\"left\">SGD(lr=0.01, momentum=0.3, nesterov=True)</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Adam(lr=0.01)</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Adagrad(lr=0.01)</th>\n",
              "      <th colspan=\"2\" halign=\"left\">RMSprop(lr=0.01)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.812417</td>\n",
              "      <td>0.730681</td>\n",
              "      <td>0.835600</td>\n",
              "      <td>0.619871</td>\n",
              "      <td>0.838667</td>\n",
              "      <td>0.621695</td>\n",
              "      <td>0.916850</td>\n",
              "      <td>0.287836</td>\n",
              "      <td>0.930200</td>\n",
              "      <td>0.233111</td>\n",
              "      <td>0.913533</td>\n",
              "      <td>0.415057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.912450</td>\n",
              "      <td>0.303032</td>\n",
              "      <td>0.919683</td>\n",
              "      <td>0.275779</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.270163</td>\n",
              "      <td>0.947767</td>\n",
              "      <td>0.196682</td>\n",
              "      <td>0.960717</td>\n",
              "      <td>0.131701</td>\n",
              "      <td>0.933817</td>\n",
              "      <td>0.465689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.928017</td>\n",
              "      <td>0.248809</td>\n",
              "      <td>0.935567</td>\n",
              "      <td>0.219742</td>\n",
              "      <td>0.936683</td>\n",
              "      <td>0.215275</td>\n",
              "      <td>0.955900</td>\n",
              "      <td>0.172627</td>\n",
              "      <td>0.969250</td>\n",
              "      <td>0.105042</td>\n",
              "      <td>0.940650</td>\n",
              "      <td>0.526929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.938883</td>\n",
              "      <td>0.212607</td>\n",
              "      <td>0.946150</td>\n",
              "      <td>0.183709</td>\n",
              "      <td>0.947283</td>\n",
              "      <td>0.180083</td>\n",
              "      <td>0.959550</td>\n",
              "      <td>0.161543</td>\n",
              "      <td>0.973900</td>\n",
              "      <td>0.089392</td>\n",
              "      <td>0.944217</td>\n",
              "      <td>0.521332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.946283</td>\n",
              "      <td>0.185853</td>\n",
              "      <td>0.954417</td>\n",
              "      <td>0.156224</td>\n",
              "      <td>0.954683</td>\n",
              "      <td>0.154643</td>\n",
              "      <td>0.962817</td>\n",
              "      <td>0.150057</td>\n",
              "      <td>0.977200</td>\n",
              "      <td>0.078644</td>\n",
              "      <td>0.946467</td>\n",
              "      <td>0.626774</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "optimizers SGD(lr=0.01)           SGD(lr=0.01, momentum=0.3)            \\\n",
              "metric              acc      loss                        acc      loss   \n",
              "0              0.812417  0.730681                   0.835600  0.619871   \n",
              "1              0.912450  0.303032                   0.919683  0.275779   \n",
              "2              0.928017  0.248809                   0.935567  0.219742   \n",
              "3              0.938883  0.212607                   0.946150  0.183709   \n",
              "4              0.946283  0.185853                   0.954417  0.156224   \n",
              "\n",
              "optimizers SGD(lr=0.01, momentum=0.3, nesterov=True)           Adam(lr=0.01)  \\\n",
              "metric                                           acc      loss           acc   \n",
              "0                                           0.838667  0.621695      0.916850   \n",
              "1                                           0.920750  0.270163      0.947767   \n",
              "2                                           0.936683  0.215275      0.955900   \n",
              "3                                           0.947283  0.180083      0.959550   \n",
              "4                                           0.954683  0.154643      0.962817   \n",
              "\n",
              "optimizers           Adagrad(lr=0.01)           RMSprop(lr=0.01)            \n",
              "metric          loss              acc      loss              acc      loss  \n",
              "0           0.287836         0.930200  0.233111         0.913533  0.415057  \n",
              "1           0.196682         0.960717  0.131701         0.933817  0.465689  \n",
              "2           0.172627         0.969250  0.105042         0.940650  0.526929  \n",
              "3           0.161543         0.973900  0.089392         0.944217  0.521332  \n",
              "4           0.150057         0.977200  0.078644         0.946467  0.626774  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "dD_mMo769r1D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "SNSzjk-59wXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dflist = []\n",
        "\n",
        "initializers = ['zeros', 'uniform', 'normal',\n",
        "                'he_normal', 'lecun_uniform']\n",
        "\n",
        "for init in initializers:\n",
        "\n",
        "  K.clear_session()\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
        "  model.add(tf.keras.layers.Dense(128, kernel_initializer=init, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(128, kernel_initializer=init, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "  model.add(tf.keras.layers.Dense(10, kernel_initializer=init, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
        "\n",
        "  model.compile(optimizer='rmsprop',  # Good default optimizer to start with\n",
        "                loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "                metrics=['accuracy'])  # what to track\n",
        "\n",
        "  h =  model.fit(x_train, y_train, batch_size=16, verbose=0, epochs=5)  \n",
        "\n",
        "  \n",
        "  dflist.append(pd.DataFrame(h.history, index=h.epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DzTZ-geS-BBR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "historydf = pd.concat(dflist, axis=1)\n",
        "metrics_reported = dflist[0].columns\n",
        "idx = pd.MultiIndex.from_product([initializers, metrics_reported],\n",
        "                                 names=['initializers', 'metric'])\n",
        "\n",
        "historydf.columns = idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "404iTWdW-B55",
        "colab_type": "code",
        "outputId": "5471b38f-43e8-4c7d-cb82-32540cfc679d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "historydf"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>initializers</th>\n",
              "      <th colspan=\"2\" halign=\"left\">zeros</th>\n",
              "      <th colspan=\"2\" halign=\"left\">uniform</th>\n",
              "      <th colspan=\"2\" halign=\"left\">normal</th>\n",
              "      <th colspan=\"2\" halign=\"left\">he_normal</th>\n",
              "      <th colspan=\"2\" halign=\"left\">lecun_uniform</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.111600</td>\n",
              "      <td>2.301552</td>\n",
              "      <td>0.909317</td>\n",
              "      <td>0.309963</td>\n",
              "      <td>0.921500</td>\n",
              "      <td>0.270653</td>\n",
              "      <td>0.929150</td>\n",
              "      <td>0.239008</td>\n",
              "      <td>0.925950</td>\n",
              "      <td>0.250398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.112367</td>\n",
              "      <td>2.301393</td>\n",
              "      <td>0.960917</td>\n",
              "      <td>0.141016</td>\n",
              "      <td>0.963900</td>\n",
              "      <td>0.130431</td>\n",
              "      <td>0.966350</td>\n",
              "      <td>0.120205</td>\n",
              "      <td>0.965483</td>\n",
              "      <td>0.124689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.112367</td>\n",
              "      <td>2.301399</td>\n",
              "      <td>0.970400</td>\n",
              "      <td>0.111161</td>\n",
              "      <td>0.972133</td>\n",
              "      <td>0.103420</td>\n",
              "      <td>0.974600</td>\n",
              "      <td>0.095839</td>\n",
              "      <td>0.973833</td>\n",
              "      <td>0.100596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.112367</td>\n",
              "      <td>2.301398</td>\n",
              "      <td>0.975217</td>\n",
              "      <td>0.095215</td>\n",
              "      <td>0.977417</td>\n",
              "      <td>0.090106</td>\n",
              "      <td>0.979183</td>\n",
              "      <td>0.081105</td>\n",
              "      <td>0.978400</td>\n",
              "      <td>0.087403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.112367</td>\n",
              "      <td>2.301411</td>\n",
              "      <td>0.979233</td>\n",
              "      <td>0.084668</td>\n",
              "      <td>0.979533</td>\n",
              "      <td>0.084339</td>\n",
              "      <td>0.982550</td>\n",
              "      <td>0.069928</td>\n",
              "      <td>0.980883</td>\n",
              "      <td>0.077935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "initializers     zeros             uniform              normal            \\\n",
              "metric             acc      loss       acc      loss       acc      loss   \n",
              "0             0.111600  2.301552  0.909317  0.309963  0.921500  0.270653   \n",
              "1             0.112367  2.301393  0.960917  0.141016  0.963900  0.130431   \n",
              "2             0.112367  2.301399  0.970400  0.111161  0.972133  0.103420   \n",
              "3             0.112367  2.301398  0.975217  0.095215  0.977417  0.090106   \n",
              "4             0.112367  2.301411  0.979233  0.084668  0.979533  0.084339   \n",
              "\n",
              "initializers he_normal           lecun_uniform            \n",
              "metric             acc      loss           acc      loss  \n",
              "0             0.929150  0.239008      0.925950  0.250398  \n",
              "1             0.966350  0.120205      0.965483  0.124689  \n",
              "2             0.974600  0.095839      0.973833  0.100596  \n",
              "3             0.979183  0.081105      0.978400  0.087403  \n",
              "4             0.982550  0.069928      0.980883  0.077935  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "ZmlNK_-k-g1H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "metadata": {
        "id": "r-YEkXkT-idc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cubZ0rQe-mBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repeated_training(x_train,\n",
        "                      y_train,\n",
        "                      x_test,\n",
        "                      y_test,\n",
        "                      do_bn=False,\n",
        "                      epochs=5,\n",
        "                      repeats=3):\n",
        "  histories = []\n",
        "\n",
        "  for repeat in range(repeats):\n",
        "    K.clear_session()\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128, kernel_initializer='normal', activation=tf.nn.relu))\n",
        "\n",
        "    if do_bn:\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "    model.add(tf.keras.layers.Dense(128, kernel_initializer='normal', activation=tf.nn.relu))\n",
        "      \n",
        "    if do_bn:\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, kernel_initializer=init, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
        "    \n",
        "    model.compile(optimizer='rmsprop',  # Good default optimizer to start with\n",
        "                  loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "                  metrics=['accuracy'])  # what to track\n",
        "\n",
        "    h = model.fit(x_train, y_train,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  epochs=epochs,\n",
        "                  verbose=0)\n",
        "    \n",
        "    histories.append([h.history['acc'], h.history['val_acc']])\n",
        "    print(repeat, end=' ')\n",
        "\n",
        "  histories = np.array(histories)\n",
        "\n",
        "  # calculate mean and standard deviation across repeats:\n",
        "  mean_acc = histories.mean(axis=0)\n",
        "  std_acc = histories.std(axis=0)\n",
        "  print()\n",
        "\n",
        "  return mean_acc[0], std_acc[0], mean_acc[1], std_acc[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MoxsGh8AAD02",
        "colab_type": "code",
        "outputId": "4b543255-cf0d-4c59-d88f-835a34477fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "mean_acc, std_acc, mean_acc_val, std_acc_val = \\\n",
        "    repeated_training(x_train, y_train, x_test, y_test, do_bn=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-3mNieHxAHE-",
        "colab_type": "code",
        "outputId": "58c693de-4e06-4faa-cc46-4dc199b70713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "mean_acc_bn, std_acc_bn, mean_acc_val_bn, std_acc_val_bn = \\\n",
        "    repeated_training(x_train, y_train, x_test, y_test, do_bn=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rWLQyScgAJWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_mean_std(m, s):\n",
        "    plt.plot(m)\n",
        "    plt.fill_between(range(len(m)), m-s, m+s, alpha=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQasvnd1AKld",
        "colab_type": "code",
        "outputId": "3b59ec02-c1f0-41dc-b440-221da6e8e0a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "plot_mean_std(mean_acc, std_acc)\n",
        "plot_mean_std(mean_acc_val, std_acc_val)\n",
        "plot_mean_std(mean_acc_bn, std_acc_bn)\n",
        "plot_mean_std(mean_acc_val_bn, std_acc_val_bn)\n",
        "plt.ylim(0, 1.01)\n",
        "plt.title(\"Batch Normalization Accuracy\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test', 'Train with Batch Normalization', 'Test with Batch Normalization'], loc='best')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe566ae23c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8FPX5wPHP7GYDCSQQIEDBW/HL\n5QFeaD1QvKooinhXiqAoNQqiUuttPVAUUUEU8Pqp9SxWpaXeUrVqkUOLIg+CoiJXIAkJR5Ldnfn9\nMbNhc7IbmCRkn/frta+de56ZbL7PzHdmvmM5joNSSqnUE2jsAJRSSjUOTQBKKZWiNAEopVSK0gSg\nlFIpShOAUkqlKE0ASimVotIaOwDVNBhjHGA5EME9MFgOXCUiP2xnvhbA+SLyXALL311EVm5nujlA\nJ+AAEYnEzy8iViLbsjN58TwJfAS8IyK967kcA3QSkY+NMWcDZ4jI8J0XKRhjHgSGAweJyC87c9mq\nedIzABWvv4h0F5H9ga+ARxKYpw8wdCfH0RK4aicvc4eIyK/1Lfw9ZwPHesv6uw+FfxowEHgA+P3O\nXLZqvvQMQNXmQ+DMWI8x5jLgOtzfzGrgEqAU+DuQbYz5RESOMcacCkwEQsBSYKiIFHiLOc0YcwXw\nG2CiiEysZd23A+ONMS+IyIaqI40x53rTpAGrgMtFZLkx5g6gK3AQ8CJQhFsolgHHAAL8Bbgf2Be4\nVUSmG2MCwGTgRCAd+BQYLiLhuHXuBSwTkTRjzF+BQ7xRLYC9gGxgc03LAU4F/gyUG2NygEXA70Xk\nRGNMO+AJL+Yo8H8icr+3Tgc3uY4FOgMTRGRSLfvsFOC/wHPAO8D4uNgPAaYDWbh/u2Ei8mMdwyud\nrcX6gf2Ae4GVQFhELq7pdyEiPxljLNzfwdlAGJiBeyb1K7C3iKz1lv0gkCYiY2rZLuUjPQNQ1Rhj\n0nGPIt/y+jsCU4CTRKQbsAy38FyLW7B97hX+rYC/4lYJ7e9Nd1fcovcSkUNwE8vdxphQLSH8DDwN\n3FlDbHvgFiZniUh34J/AtLhJTgNOE5GHvf5TvOV0A3oAN+AmgxHArd40Z3vDenvTHAKcX9v+EZGL\nvTOl7sD7wGQRKaltOSIyCzdRPiIi11VZ3L1AoYgY4Gjgj8aYo+PG9xKRPrj77F5jTLCWsIYBz4vI\nr8BaY8xhceNeBm7x/iZ/x/1b1jW8Ln2AJ7zCv8bfhTfdxcDhwP7AocDVuH+D96m8b8/24lCNQBOA\nijfHGLMEWAscBjwDICLrgOy4+vtPgH1qmP+3wC8i8o3XPw64Nm78C973Qtxqng51xHIfcIYxpleV\n4ScBH4nIMq//SeB4rwoE4L8isj5u+sUislREyoDvgXdFJIp7FN7F276ZwKEiEhaRUuDLWravEmPM\nENz9dMMOLOd0YKo3fwHwOnBy3Pjnve8FuPusYw1x5OAmmw+9QS/gVcsZY/YHOojIv7xxU4Bzahu+\nvW0GtorIh168df0uTgP+5u2LYtyE+CXwEnChF9uBQFBEvkhgvcoHWgWk4vWPO+0/Fvi3MaYvsA74\nizHmTCCIW2WwtIb5O+BWuwAgIuVVxhd7w6PuNVFqO5pFRDYbY24DHsI9io/JBQrjptvoVTfEkkkB\nlZXEdUeBTXHdAW9bc4HJ3rbauNUtD1MHY8ye3jQnesmlXsupuj1ed5e4/o3edta1zy705inwprGA\nMmPMWNz9sjE2oXdhPWKMqXH4dmKFuP3rnY3U9ruo+lvY7M3zFjDDGLM3cBbwagLrVD7RMwBVIxH5\nGPgJt1rifNwqiGO9qorba5ltPXFH9caYTGPMbjsQxnNAO2PMwLhha4H2cevIwS1s11N/9+DWUx8Q\nV61UK6/gexG4Q0SW1Hc5nkrb43WvTSJ2gD/gJu+23qcN8Dnu2cV63H0YS3Yh73pGbcPB3Z9Bb3hO\nHeut63dR9bfQyRiT7SWCWcC5wBDglSS3Ve1EmgBUjbwqAgMswa12WCEi640x7YHzgNbepGHci8AW\n7kXPznH1z7cCt9U3BhFxgDG4FxNj3gOONcbEqhquxK3WSeTotTYdgUUiUmaMOQi3Kqt1HdPfAawU\nkSeTWE4YaFvDsv4BjATwjsoHk1jiwJunB+4F2v9WGfUGbjXQ97gXbQd7w0fgXvitbTi4F3MP8rqH\n4yaEmtT1u3gLuNAY08K7NvQp7rURcJPnH4FMEZmf6LaqnU8TgIo3xxizxLsO8BpwhYgswq23bW+M\nWeZ13wLsboyZiPuP3QX3bpwy3HrkF4wxS4EDgZt2JCAR+Q/uLamx/pXAZcCbXpzHAlfsyDpwE8yV\nxpjvcG8/vQ64zLvbqCY3Af1i+8r7HL2d5czyxv2tyrJuAXK8bfkYuE9E5iYR+x+At7xkGW8WbtVZ\nDu7R9s3GmO+Bi4BR3vTVhnvz3gw8boz5CvfOpuJa1l3X7+IV3LuRvse95vOUiHzmzfcO7l1TevTf\nyCx9H4BSqqEZY74FzhWRxY0dSyrTMwClVIMyxlwArNbCv/HpXUBKqQZjjHkP9+LwkMaORWkVkFJK\npSxfzwCMMb2BN4FJIjKlyrgTcZ+CjAKzReSuGhahlFLKJ74lAO/Wr8nAB7VM8ijuXQq/4j5wNLOu\nOsH8/JJ6n6rk5GRSWLilvrP7pqnGBU03No0rORpXcppjXLm5WbW2ouvnReAy3MfBV1Ud4d3DXSAi\nv4iIDcwGBvgVSFparQ+cNqqmGhc03dg0ruRoXMlJtbh8SwAiEhGRrbWM7gzkx/Wvw20hUimlVANp\nKncBbfdFHzk5mTuUBXNzs+o9r5+aalzQdGPTuJKjcSUnleJqrASwCvcsIKYrNVQVxduRernc3Czy\n80u2P2EDa6pxQdONTeNKjsaVnOYYV12Jo1ESgIisMMZke41PrcR9acfFjRGLUqppcRwHHAdsG8ex\nwXZwbBtsG5xt3Y5tY9tR7KiNE7WxozZ2NIoTjWLbDnY0im3b2BH324nNE41iRx2IbhtuR93vlS3T\n2LyptGIZ2FFvPsedNmpXGubYNjjuvERtnCrxueMcL3Z3WyqGOXaV4fHDvG7vNv09Bp9Fl8P67PR9\n7eddQIfgto2yFxD22k5/C/hRRP6O2+7IS97kr4hITc0LK9VsVSoo7KhXuNg4sW7HLbycaJRoJIod\niRKNut+2N8yJusOcuOF2JFpR0K1MD7KpZKtXeEXjPl6BGI1W+ibW73VXFMJewbctXhsrVljF93vD\nrIp+p1J3wHG7La+As3CwHO/jdQdovGeTtuDWR1s0nWYSHOCXRUt3rQTgtfLXv47xHwNH+rV+lboc\nx8GJhHHCEZxIBCccdvsjESKlZZSXl1G+tYzyslLCZaWEy8oJl5cRLS8jUl5ONFyOXR7GjpR7y3Dn\nJxJxP9EIVjSKFY0SiESx7CgBxwHbIWDHCrq4gs1xCNjetxP33QD7YgtuQeZXYRZfVNtY2JaFY1nY\nFkQtC8cKeMPAtizsgDcsEJvWG48FVoBoIIDjLSM2DitQ0e8u2y2ibSsAccO3fbsfrIC33G3jqFiX\nNw2Bim7HsrACQffgO34aK5YSrErLsOKHVXRT0V1pPuKHA078eLZN42zrx+u3LIv+A7r78vdrKheB\n1S7IdmyidpSIEyViR4iWl7kFabiMaLicaLn7bYfLiZaXEw2XES4LE4kVtOXl2OXl2JGIW+CGw9jh\nCETCOBEbIt4pd9SGqAM2BGwgamPZuP2OBTZYjlXp41gBHALYsUKhotsd7hYkgcrTESsoAjhWEJtW\n2/qpMn0wgJMWwG4ZWxdsKwC87opOq1JBua2Q8Ppj3bH/eaqMt2pKFfHLrGV8tcE1Txe30u1PB17B\ntwtxoBFPKnaYbdnM//479tuz605ftiaAXVzUjlJuhwnbYcqjse9ywnaEcDRMuV1OOBqmLFJOaThM\neXmE8nCYcHmE8kiEcFkYSkoJFJdiFZcT2BImWBolELVwvIIVx/u2vaMeJwBY7tEJAa9EixWigYpC\ns1KhGl8AWyFsWsQVroGKo0HHqnKnl4X7K21Kv1TLwT3AtLAC7ncgYFHRrIoVV0R6HYEq/dWmqzK+\nSo7wOq0ay+ZK+SFuxZYFwUCAqG3XMq1VeZaaVhEXSNWYti3CqtJfdXmVExxAenqQSMSuOFC2LKvi\ngLimbrff298Agfjh8dNVnr5i+YG45RI33Jsn1t+qdQu2bnVfZFexrkrLseLGWRBw07cViF/mtu2t\naVscy6ljGyvHY1nu/9wh+3SnqKCUna0p/Vs1G7FCuXKBvK0/bIcpC5cT2hBkw4YSysMRysPlhMNR\nysIRIuEI4YhNNBwlErGJRhyiURs7AnbEwYlS8bHsAJYdJGAHsexA5W8nSCDqdVeqBAh5nzoEqeOF\njUlwHNz3iThYVuxQzNl2hozj/hNZDpZlYwWiBAKWW7gGAwQCAQJpQQLBAMG0IBmZLbBtCKUFSUsL\nkp4WJBQKev0BgsEAgaBFMOB+B7zCORAMVCw3ELAIBi132QGrYjp3XMAbV/N8gYC17R88TnO8e8RP\nGldyQsEQoAmg3txCuZzyaISwXR5XOIcpt90j5PKyMKXh8oqj5Nh3JBIl7BXGkbBNNBL7ONhRcKLx\nhbJVvSC2gwTihzlBLKdqrWx6jXEnVw67hWjAiRJwogSj5QSjYYJOhKAdJeBECDpRAnaEgBPFtqDc\nChAOpGGnp2NlZBBqnUV622yy2rYiYkNaehpp6SFC6WmE0gKEQmmkhwKkhQKkh4LbPulB0kNeARxX\n+NZUWO6IpvoPqtSuqNkngIUrlvDe9K+xIlULZLdQjhXOVrXLZBa1HSknXig7EHSwgmAFIZAOVpp7\n9BlMs2jRMg0H9zHvUChAWiiN9FAaoVCQFukh0kNptEhPJxSwsLZuxinZyNaCAso3bMAu2oC1sYDQ\npkJalG8haEewsCudoZcGQmxMa83G9CzKW7eFtu1I65BLZqeOtNntN3TokE1u2wxatUyrVlBrQatU\n89fsE4CzKY2swo7gWG7dW2BboRxIB8srjINpEEgLkBb7eNUKofgCOT2N9FB6ReEcCgUJxk0fmzeY\nFiQtFNjuEXCskHWiUSKFBZTl51P86xo2/byG0vx12AXrcYoLsUo3A25KyvQ+AGEryMa01qxtmUtZ\nqzbYbdoRbN+BjI4dyd6tM+07tWP3Nhm0aZ1OYCcfiSuldn3NPgH07b0fA44+kPUbNhEMNs6dvY7j\nEN24kfL8dZSsWkvJr2vYum4dFBXgFKwnfWuxexthnJa4t9UVp7ViTUZntmRkE81uR7Bde1p07EhW\nl06069KR/dtmkJPdgmCgqdy1rJTaVTT7BACQFgr6XvhHN28mvD6fTavWUvzrarasXUtk/XqsogLS\nNxcRtKOVpm/hfZcEM1jfogObW2YTycrBymlPem4urbt0pl3XjuzRrjXts1sQaqKtFCqldl0pkQB2\nBrusjPCG9Wxds5ailavZsmYt5fnroXADoU1FhCJllaZP9z5bA+nkh9pQkp5FuHVbyGlPqEMumb/p\nxB499qJTejo927Qko4X+KZRSDUtLHY8TiRAuLKBs7TqKVq5m8+q1lOWvwynYQLCkkBZllRuji92a\nHraCFIVaU9wql/JWbXHatiOtQwcyOnekTdff0KFTDvu20QutSqmmJ2USgGPbRIs3UrYun40rV7Np\nlVsPbxesJ7ixkBZbSyo9WxkAMoAoFsWh1qzO+A2lrdpgt8kh2K4DLTt1JLtLZ9r9JpdeORlkt9IL\nrUqpXUuzTwD5S5bx9fXTSC8uqFYPn4H7WFJJMJP8lrnehdYcAu060LJjLq26dKZ9l07s2y6TnCy9\n0KqUal6afQJYuXID4U1bKQq1ZXPLbMJZbSsutLb6TWdyduvM7u2zOCi7JaE0LeCVUqmj2SeAgwcc\nDqceRdmWMlqmN/vNVUqphDX7Q17LsuiYk6mFv1JKVdHsE4BSSqmaaQJQSqkUpQlAKaVSlCYApZRK\nUZoAlFIqRWkCUEqpFKUJQCmlUpQmAKWUSlGaAJRSKkVpAlBKqRSlCUAppVKUJgCllEpRmgCUUipF\naQJQSqkUpQlAKaVSlCYApZRKUZoAlFIqRWkCUEqpFKUJQCmlUpSvL8o1xkwC+gEOMFpEvowbdxXw\neyAKzBORMX7GopRSqjLfzgCMMccB3UTkSGAE8GjcuGzgBuAYETka6GmM6edXLEopparzswpoAPAG\ngIh8B+R4BT9AufdpbYxJAzKBAh9jUUopVYWfCaAzkB/Xn+8NQ0RKgTuBH4CfgP+KyFIfY1FKKVWF\nr9cAqrBiHd6ZwE3A/kAx8KEx5iAR+bq2mXNyMklLC9Z75bm5WfWe109NNS5ourFpXMnRuJKTSnH5\nmQBW4R3xe7oAq73uHsAPIrIewBjzCXAIUGsCKCzcUu9AcnOzyM8vqff8fmmqcUHTjU3jSo7GlZzm\nGFddicPPKqB3gSEAxpi+wCoRiW3BCqCHMSbD6z8U+N7HWJRSSlXh2xmAiHxmjJlvjPkMsIGrjDHD\ngI0i8ndjzAPAR8aYCPCZiHziVyxKKaWq8/UagIjcWGXQ13HjpgHT/Fy/Ukqp2umTwEoplaI0ASil\nVIrSBKCUUilKE4BSSqUoTQBKKZWiNAEopVSK0gSglFIpShOAUkqlKE0ASimVojQBKKVUitIEoJRS\nKUoTgFJKpShNAEoplaI0ASilVIrSBKCUUilKE4BSSqUoTQBKKZWiNAEopVSK0gSglFIpShOAUkql\nKE0ASimVojQBKKVUitIEoJRSKUoTgFJKpShNAEoplaI0ASilVIrSBKCUUilKE4BSSqUoTQBKKZWi\nNAEopVSK0gSglFIpShOAUkqlKE0ASimVojQBKKVUikrzc+HGmElAP8ABRovIl3HjdgdeAtKBBSJy\npZ+xKKWUqsy3MwBjzHFANxE5EhgBPFplkonARBE5HIgaY/bwKxallFLV+VkFNAB4A0BEvgNyjDHZ\nAMaYAHAM8JY3/ioR+dnHWJRSSlXhZwLoDOTH9ed7wwBygRJgkjHmU2PMeB/jUEopVYPtXgMwxnQX\nkSU7YV1Wle6uwCPACuCfxpjTReSftc2ck5NJWlqw3ivPzc2q97x+aqpxQdONTeNKjsaVnFSKK5GL\nwDONMYXAU8ArIrIlwWWvYtsRP0AXYLXXvR74SUSWAxhjPgB6AbUmgMLCRFdbXW5uFvn5JfWe3y9N\nNS5ourFpXMnRuJLTHOOqK3FstwpIRHoBVwJ7A3OMMdONMYclsN53gSEAxpi+wCoRKfGWGQF+MMZ0\n86Y9BJAElqmUUmonSegagIh8IyK3AWOBHsBbxpiP4wrwmub5DJhvjPkM9w6gq4wxw4wxZ3uTjAGe\n8cZvBGbtyIYopZRKTiLXAPYEhgEXAouBe4B3gMOAF4AjaptXRG6sMujruHHLgKOTjlgppdROkcg1\ngDm49f8niMiquOFzjTFzfYlKKaWU7xKpAjoIWBor/I0xVxpjWgOIyNV+BqeUUso/iSSAZ6h8N08m\n8Lw/4SillGooiSSAdiJS0YyDiDwEtPUvJKWUUg0hkQTQwhjTI9ZjjDkEtwE3pZRSu7BELgJfC7xp\njGkDBHGbdLjE16iUUkr5LpEHwf4rIvsDPYH9RaQHegaglFK7vESeA8gGfg908PpbAJfiNu2glFJq\nF5XINYBXgANxC/0sYCAwys+glFJK+S+RBNDSe1vXTyJyA3A8cJ6/YSmllPJboncBtQICxpj2IlIA\n7OtzXEoppXyWyF1AzwGXA08C3xlj8oHvfY1KKaWU7xJJANNExIGKdvs7Al/5GpVSSinfJZIAPsSt\n90dEfgV+9TUipZRSDSKRBPCVMeYvwGdAeWygiHzoW1RKKaV8l0gCONj7PiZumIN7ZqCUUmoXtd0E\nICLHN0QgSimlGlYiTwJ/gnvEX4mIHOtLREoppRpEIlVAt8R1pwMnAJv8CUcppVRDSaQK6N9VBr1n\njJntUzxKKaUaSCJVQPtUGbQ7YPwJRymlVENJpArog7huBygG7vAlGqWUUg0mkSqgvY0xARGxAYwx\nIREJ+x+aUkopP223MThjzDnAm3GDPjHGDPEvJKWUUg0hkdZAr8N9IUzMyd4wpZRSu7BEEoAlIhtj\nPSJSDNj+haSUUqohJHIReJ4x5hVgDm7COBWY72dQSiml/JdIArgGuBg4AvcuoBeA1/wMSimllP8S\nSQCZQLmIXA1gjLnSG6ZPAyul1C4skWsAzwGd4/ozgef9CUcppVRDSSQBtBORR2M9IvIQ0Na/kJRS\nSjWERF8K3yPWY4w5FLdROKWUUruwRK4BXAu8aYxpg5sw1gOX+BqVUkop3233DEBE/isi+wOH4j4A\ntgp4y+/AlFJK+SuR1kD7AZcC5+MmjJHATJ/jUkop5bNaE4AxZhwwDGiFeyfQocBrIvJyogs3xkwC\n+uE+PzBaRL6sYZrxwJEi0j+pyJVSSu2QuqqA7gHKgWEicquILKOGV0PWxhhzHNBNRI4ERgCP1jBN\nT0BfLamUUo2grgSwO/AS8IQxZpkx5haSu/tnAPAGgIh8B+QYY7KrTDMRuDmJZSqllNpJaq0CEpE1\nwP3A/caYY4HhwJ7GmFnA4yKyvddCdqZym0H53rBiAGPMMODfwIpEAs3JySQtLZjIpDXKzc2q97x+\naqpxQdONTeNKjsaVnFSKK5HbQBGRj4GPjTFXAxcBtwHJvhfYinUYY9rhXlg+EeiayMyFhVuSXN02\nublZ5OeX1Ht+vzTVuKDpxqZxJUfjSk5zjKuuxJFQAogRkRJgmvfZnlVUbkKiC7Da6z4ByAU+AVoA\n+xpjJonItcnEo5RSqv4SeRK4vt4FhgAYY/oCq7wEgoj8TUR6ikg/4GxggRb+SinVsHxLACLyGTDf\nGPMZ7h1AVxljhhljzvZrnUoppRKXVBVQskTkxiqDvq5hmhVAfz/jUEopVZ2fVUBKKaWaME0ASimV\nojQBKKVUitIEoJRSKUoTgFJKpShNAEoplaI0ASilVIrSBKCUUilKE4BSSqUoTQBKKZWiNAEopVSK\n0gSglFIpShOAUkqlKE0ASimVojQBKKVUitIEoJRSKUoTgFJKpShf3wjW3E2ePAmR7ygo2EBpaSld\nunQlO7sN9977QJ3zzZ49i1atWjNkyJkNFKlSSlWnCWAHXH21+x772bNn8cMPy8nLG5PQfKeddoaf\nYSmlVEKaTQJ49cNlfLlkXY3jgkGLaNRJepmHde/IeSfsl9Q8CxbM4+WXX2DLli3k5V3LwoXzmTPn\nA2zb5sgjf8vw4SN56qlptG3blj59DuDpp5/FsgL89NOP9O8/gOHDRyYdp1JK1UezSQBNyfLly3jp\npddJT09n4cL5TJ36JIFAgPPOG8T5519UadrFi7/lxRdnYts25557hiYApVSDaTYJ4LwT9qv1aD03\nN4v8/JIGi2W//bqRnp4OQMuWLcnLG0kwGKSoqIji4uJK0xrTnZYtWzZYbEopFdNsEkBTEgqFAFiz\nZjWvvPJXnn76r2RmZnLJJedVmzYYDDZ0eEopBehtoL4qKioiJyeHzMxMRJawZs0awuFwY4ellFKA\nJgBfdeu2PxkZmYwaNZwPPniXQYMGM3Hi/Y0dllJKAWA5TvJ3xzSG/PySegfa0NcAEtVU44KmG5vG\nlRyNKznNMa7c3CyrtnF6BqCUUilKE4BSSqUoTQBKKZWiNAEopVSK0gSglFIpShOAUkqlKH0SeAfU\ntznomJUrV/Ljj7/SvXtPnyNVSqnqNAHsgPo2Bx3z+eefU1S0SROAUqpR+JoAjDGTgH6AA4wWkS/j\nxh0PjAeigACXiYhd33W9vuwfLFy3qMZxwYBF1E7+ObI+HQ9g8H4Dk55v6tRH+fbbRdh2lCFDLmTA\ngJP4/PP/8PTT00hPb0GHDh246qoxTJ06lUAgjY4dO3PUUUcnvR6llNoRviUAY8xxQDcROdIY0wN4\nGjgybpLpwPEistIY8xpwKjDbr3gayoIF8ygsLOCxx2ZQVlbKiBFDOeaY45g58xVGj76e3r0P5KOP\n3icUCnHmmWfSunVbLfyVUo3CzzOAAcAbACLynTEmxxiTLSKx9pAPievOB9rvyMoG7zew1qP1hny8\ne9Gir1m06Gvy8tx2/W07SkHBBo4//kTuv/9uTj75NE466RRycto1SDxKKVUbPxNAZ2B+XH++N6wY\nIFb4G2N+A5wM3FrXwnJyMklLq3/Tybm5WfWed3uyslqSmZlObm4Wbdu25sILL+Cyyy6rNM0BB+zP\nwIGn8P777/PnP49lypQpALRu3dLX2HaExpUcjSs5Gldy/IirIS8CV2uQyBjTEZgF/FFENtQ1c2Hh\nlnqv2O8zgJKSUrZsKSc/v4Q99+zGjBmPc8YZ51JeXs4TT0xhzJjreeaZGZx77oUMGHA6K1asZOHC\nbwgEAmzcuLnZNT7lJ40rORpXcppjXHUlDj8TwCrcI/6YLsDqWI8xJhv4F3CziLzrYxwN6uCD+9K7\n94FcccWlgMM555wPQG5uR6655kqysrJp06YNv//9H2jfPpsbb/wzbdq05cQTT2ncwJVSKcfPBPAu\ncCcwzRjTF1glIvEpbCIwSUTe9jGGBnHaaWdU6h816upq0wwcOIiBAwdVGnbsscfy5pu7/OYrpXZR\nviUAEfnMGDPfGPMZYANXGWOGARuBd4ChQDdjTKyy/EURme5XPEoppSrz9RqAiNxYZdDXcd0t/Fy3\nUkqpumlbQEoplaI0ASilVIrSBKCUUilKE4BSSqUoTQA7YPLkSeTljeSii85h8ODTycsbyU033ZDQ\nvLNnz+K9997b4RgeeWQiq1b9yubNm5g79wsAnnpqGjNnvlLnfMcddwR5eSPJyxvJ5ZcP5d///qjO\n6T/99N+Ew+Fax59++oA653/qqWlcdtlQHGdbo3yx5jL8FIsrtp+SEdvmDRvWM2HCPX6Ep1Sj0uag\nd8CONAd92mln7JSnDkePvg5wG6GbO/cLDj+8X0LztW7dmilT3Ltu16xZw7XX/pHjjju+1ulffvmv\n9O17GKFQqN6xhsPlfPjhewwYcHK9l1Ffsf2UjNg2t2/fgXHjbvYhKqUaV7NJAPmvvUzJvC9rHPdT\nMEA0mnxL01mHHkbuuRckPd8Yw0JkAAAUIElEQVSCBfN4+eUX2LJlC3l517Jw4XzmzPkA27Y58sjf\nMnz4SJ56ahq77daZ3NyuvP76q1hWgJ9++pH+/QcwfPi2I+Pp06ey7777MWDAyTzwwL0Eg0HGjv0T\n7733Nr/88jMLFsxj7NhxPPTQBLZs2czuu+8BwA8/LGfcuDH88svPjB59Pf36HVVrvIWFG8jN7QjA\nunVrueuu2wiFgmzdWsYtt9zJokVfs3jxN1x//TU88sjjvPrqi8yZ8wGWFeDKK/Po2/dQAJ588gnm\nzv2CNm3acP/9kwgEKp9gDh06nOeff5bjjjuBtLRtP71NmzZxzz13sGlTCZFIhDFjbsCY7lxwwdns\nv393Dj/8CN5+ezZ9+x7KV1/NIxp1+N3vTmf27H8QCAR45JHH2bBhPXfddRsAkUiEW265k65dd6tY\nR17eSMaOHcdHH33AwoXzK/bRtdfewEEH9ak2b/w233jjrdx55y089dTzLFgwj+nTp5KWlkZubkf+\n/OfbeP/9d1i69FvWrFnHzz//xEUXXcLAgWcl/btRqqFpFZBPli9fxkMPTaF79x4ATJ36JNOnP8u/\n/vUPNm/eVGnaxYu/5eab7+CJJ56pVnXTp09fvv3Wfc9BQcEG1q1bC7itjsYKXoCLLrqEE044iUGD\nBgOwcWMREyY8zJgxN/DmmzOrxbdp0yby8kYyatRwxo27lmHD3OfxNmxYz6WXXs7zzz/P6aefyeuv\nv8app55Ou3btefDBR1mzZjVz5nzAtGnPctttd/Huu/8CoLi4mP79BzB9+rMUFxezfPn31daZk9OO\nY445jjfe+Ful4a+99hK9evVm8uRpjB59HZMnPwTAqlW/MmzYZRWFafv2HXjppZew7SjFxcVMnfok\ntm3zww/LKuKePHlaRdw1GTHiCqZMmc4114xljz32pH//ATXOG7/N8Wc9Dz44njvvvJcpU6aTlZXF\ne++5T3IvXbqUe+55gPHjJ/K3v71a47qVamqazRlA7rkX1Hq03hgNPO23XzfS09MBaNmyJXl5IwkG\ngxQVFVFcXFxpWmO607JlyxqX07v3Qfzf/z1NcXExmZmtiEQilJaWsnSpkJd3ba3rP/DAgwHIzc1l\n06ZN1cbHVwFt2LCe0aP/yNSpM2jXrj0PP/wgzz33JAUFhbivcthm6VKhZ8/eBAIBdtttd2680W3E\ntVWrVuy3X7c61wlw4YWXcOWVl1ZqPmPJksUMHToCgO7de7Jy5S/efstgn332rZiuZ89egJsIunUz\nALRr145NmzbRpUtXHn74QZ56aholJcXV4o5XWlrK/fffw+23300oFKrY5u3NW1y8Ecuy6NTJbeLK\nPSNZwP77d+fggw8mGAySm9uxWoJXqqlqNgmgqYkdNa5Zs5pXXvkrTz/9VzIzM7nkkvOqTRsM1t7M\ndUZGBoFAgIUL59Or1wGUlpYyb95cMjIyKhJMTeKXGX/htSbt23dg7733Ydmy73n77X9yxBH9uPzy\nS3n11b/z2WefVlluALuGt6tV3Yba1pmZmcmgQefw4ovPVwyzLKvS9LbtVteFQpV/nvHrqLp9Tz01\njSOO6MdZZw3ho4/erxZ3vEceeZCzzx7CHnvsCZDEvJXjDIfDWJZ7Eh1fpbW9/a1UU6FVQD4rKioi\nJyeHzMxMRJawZs2aOu+mqUnPnr15/fXX6N37AHr1OoCZM1/hoIP6VJrGsiyi0Wi9YiwvL+eHH5bR\ntetuFBUV0bXrbjiOU+nOH8sKEI1GMaYHixZ9TSQSoaBgA3/+8/VJr2/QoMF8+unHFBYWAO5R/8KF\n8wD45ptF7L33vnXNXqPa4q5qzpwP2Lx5c6WG+ba3zTHZ2dlYlsWaNWsA+OqrBRVVfErtivQMwGfd\nuu1PRkYmo0YN54ADDmbQoMFMnHg/Bx54UMLLOPjgvsyc+Sr77tuNSCTMV18tYNiwyytNY0x3nnhi\ncsXF3O2JXQMAKCsr5bzzLqJTp84MGjSYSZMeYObMlznzzCFMmHAPc+d+QZ8+ffnjH0cwefJ0Tjnl\nNPLyRuI4DldccVXiO8OTlpbG0KHDue02t6mo8867kHvvvZNrrrkS27YZO/ZPSS8zFnfnzl0YMuT8\nirirmjbtMTIyMiu2/fjjB9Q6b2ybb775jor5x427hTvvvJlgMEjXrrsxYMDJFddBlNrVWLvK6Wp+\nfkm9A22OL3nwW1ONTeNKjsaVnOYYV25uVrWXccVoFZBSSqUoTQBKKZWiNAEopVSK0gSglFIpShOA\nUkqlKE0ASimVojQB7IAdaQ46ZvXqVSxZsjipeSZNmsCaNWsoKSnhyy//C7iNxlVtYydeJBKhf/9+\n5OWN5Oqrr+Dyy//Ap5/+u871fPzxHCKRSK3LO/PMU+qcf/r0qYwcOazSML+bgI6PK7afkhHb5vz8\ndTz44H1+hKhUk6EPgu2AHWkOOmbevLlEoxG6d++Z8DzXXjsOgC+//C/z5s3lsMOOSGi+7Ow2Fe3/\n/PrrSv70p7EcffRxtU7/0kvPc8QRR1Zq5iBZZWWlzJnzAf371/2+AD/E9lMyYtucm9uR66+/0Yeo\nlGo6mk0C+OzD5fywZF2N4wLBAHY9moPep3tHjjoh+WYJAKZOfZRvv12EbUcZMuRCBgw4ic8//w9P\nPz2N9PQWdOjQgdtuu4Vnn32SUCidjh07c9RRRwPw+OOT6dGjJ/37D+C+++4iIyOT0aOv4+23/8na\ntWv44ovPuPHGW3noofspKytj9913B2DZsu8rmoAeO3Ychx1W+7sBCgsLyM3NBdz3Adx9921YlkUk\nEuHWW//CJ58sZsmSxYwdm8ejjz7Biy8+x8cff0QgEGTUqKvp3ftAwH2ydt68ubRr14777nsIy6r8\nzMkf/nAZzz33NEcffVylRFJcXMz48XdSUlJCNBpl7Ng/sffe+zB06Pnss89+HHXU0cya9QaHH96P\nuXM/Jy0txEknncr77/8LywoyadJjrFu3rlrcHTt2qljHqFEjuPHGW3nnndn8739fAW4rrTfccBM9\ne/auNu9XXy2o2OY//elm7r77DqZPf5b5879k+vSphEIhOnXqxI033sbbb/+TxYu/paBgPb/88jOj\nRl3JMcecVK/filKNRauAfLBgwTwKCwt47LEZPPzwVJ55Zgbl5eXMnPkKo0dfz2OPzaB//wGEQiFO\nOeU0LrjgoorCH9ymH7799hvAbdZ59Wr3TVbVm4AeykknnVrRXHJJSTETJjzM1VeP5c03X68WV3Hx\nxoomoG+66YZKTUCPGHEFkydP49RTT+eNN2YyePBg2rbN4aGHprBy5S98+unHTJv2LDfffHtF0wdF\nRYWcfPLvmDHj/9iwYQM//ri82jrbt+9Av36/ZdasNyoNf/XVFznwwD5MmTKdq64aw5QpkwBYufIX\nRo4cVdFaaG5uRx5//GlKS0vZunULL730EqWlpaxY8UONcddk5Mg/MmXKdPLyxrDXXntz7LH9a5z3\ntNPOqNjmQMBtbM5xHB54YDz33DOBKVOm07JlBh988C4AP/64nPHjJ3L33ffzwgsv1PmbUKopajZn\nAEedsG+tR+sN/Xj3okVfs2jR1xX13bYdpaBgA8cffyL33383J598GieddArt27evcf6DDjqYF198\njsLCQrKystm6dStlZWV8//3SOqs1ttcEdHwV0Pr16xkzZhSPP/407du355FHHuTJJ5+guHgjvXod\nUGk+kSX06uU2Ab3HHnsxbtzNRCIRsrKy2HvvfepcJ8DFFw9l1KgRnHrq6RXDlixZzGWXjQKgd+8D\n+PnnnwBo1ao1e+yxV8V0sSagO3So3gR0x46d6ow73pYtW5gw4V7uuus+0tLStrvNMUVFhaSnh+jQ\nwT1b6tv3UBYv/oa99tqH3r0PJBAIkJvbqdZtV6opazYJoCkJhUKceebZXHTR0ErDTz/9TI488rd8\n/PEcbrhhNE888XiN82dmtsJxHL76ym0Curi4mPnzvyQrK7vO+vjKTSTXHWOHDh3YY4+9+OGHZbz1\n1uscddQxnHHGWbz//jvMmze3ynJrawK6ciy1rbNVq9YMHHgWL7+87SjZrSpyvPkcbNttdbPqKyfj\n1xHf7TgwY8bUOuOO9/DDD3DuuRdUvCUs8XlrbwI6mSa3lWqKtArIBz179uY///kE27YpLS3l4Ycf\nBOCZZ2aQnt6Cs846h/79B7B8+XICgUCNzTj36NGLN954nd69D6RXr9689tpLHHxw30rT7EgT0GVl\nZfz443KvCeiNdO26G7Zt88kn8c0hu8vv3r0n//vfQqLRKOvXr+eWW5K/uHr22UOYM+dDioqKALcJ\n6AUL3Cag//e/r9l33/2TXmZtcVf1wQfvEg6H+d3vBm533qr7NCcnh0gkUvEmNm0CWjUnegbgg4MP\n7kvv3gdyxRWXAg7nnHM+4NZnX3PNlWRlZdOmTRvGjMljy5YI48f/hTZt2nLiiadUWsasWX9n7733\noaysjIUL5zNixJWV1mNMD2bMmFpxMXd7YtcAwL075+KLh9KhQy5nnTWYiRPvo3PnLgwefC4TJtzD\n559/Tp8+h3DFFcN47LEnOeGEk7jqKrcJ6iuvzEt6n4RCIX7/+z9w9923A3D++RczfrzbBLTjOFx3\nXfJ33NQU9/z51d8LPW3aY2RlZVds+4ABJ9c477x5cyu2+aabbq+Yf9y4m7n99psIBoPsvvseHH/8\nicyePSvpeJVqarQ56EbUVOOCphubxpUcjSs5zTEubQ5aKaVUNZoAlFIqRWkCUEqpFKUJQCmlUpQm\nAKWUSlGaAJRSKkX5+hyAMWYS0A/3kc/RIvJl3LgTgXuBKDBbRO7yMxallFKV+XYGYIw5DugmIkcC\nI4BHq0zyKHAO8FvgZGNM4u0hK6WU2mF+VgENAN4AEJHvgBxjTDaAMWYfoEBEfhERG5jtTa+UUqqB\n+FkF1BmYH9ef7w0r9r7z48atA+pseL+up9kSkZubtSOz+6apxgVNNzaNKzkaV3JSKa6GvAhcVwG+\nQ4W7Ukqp5PmZAFbhHunHdAFW1zKuqzdMKaVUA/EzAbwLDAEwxvQFVolICYCIrACyjTF7GWPSgIHe\n9EoppRqIr62BGmPuA44FbOAqoA+wUUT+bow5Frjfm3SmiDzoWyBKKaWq2WWag1ZKKbVz6ZPASimV\nojQBKKVUimp2r4Rsqs1PbCeuFcAvXlwAF4vIrw0UV2/gTWCSiEypMq4x91ddca2g8fbXBOAY3P+d\n8SLyety4xtxfdcW1gkbYX8aYTOBZoBPQErhLRP4RN75R9lcCca2gkX5f3vozgG+8uJ6NG77T91ez\nSgDxzU8YY3oATwNHxk3yKHAK8Cvwb2PMTBFZ3ATiAvidiGzyO5YqcbUCJgMf1DJJY+2v7cUFjbO/\njgd6e3/H9sBC4PW4SRprf20vLmiE/QWcAcwTkQnGmD2B94B/xI1vlP2VQFzQOPsr5hagoIbhO31/\nNbcqoKba/EStcTWyMuA0angGo5H3V61xNbKPgXO97iKglTEmCI2+v2qNqzGJyCsiMsHr3R1YGRvX\nmPurrrgamzGmO9AT+GeV4b7sr2Z1BsBObn6igeKKecIYsxfwKfBnEfH99iwRiQARY0xNoxttf20n\nrpjG2F9RYLPXOwL3NDxWTdCY+6uuuGIafH/FGGM+A3bDfd4npjH/H+uKK6ax9tdEIA/4Q5Xhvuyv\n5nYGUFVTbX6i6rpvA8YC/YHeuK2kNjVNqbmORt1fxphBuAVtXh2TNfj+qiOuRt1fInIUcCbwgjGm\ntv3S4PurjrgaZX8ZY4YCn4vIjwlMvlP2V3NLAE21+Ym64kJEnhORdd6R72zggAaKqy5NtrmOxtxf\nxphTgJtx64g3xo1q1P1VR1yNtr+MMYcYY3b3YvgKt8Yh1xvdaPtrO3E15u/rdGCQMeYL4DLgVu/C\nL/i0v5pbAmiqzU/UGpcxpo0x5h1jTLo37XG4dwA0qqbaXEdj7i9jTBvgAWCgiFS6SNeY+6uuuBr5\n93UscJ0XRyegNbAeGv33VWtcjbm/ROR8ETlMRPoBT+LeBfS+N24FPuyvZvckcFNtfmI7cY3GrfPb\ninsHx9UNUedojDkEt85xLyCMe3fBW8CPjbm/EoirsfbXSOAOYGnc4A+BRY28v7YXV2PtrwzgKdwL\nrRnAnUB7Gvn/MYG4GmV/VYnxDmCF1+vb/mp2CUAppVRimlsVkFJKqQRpAlBKqRSlCUAppVKUJgCl\nlEpRmgCUUipFNbemIJRKive4vwCfVxn1TxF5YCcsvz9wt4gcvaPLUmpn0wSgFOSLSP/GDkKphqYJ\nQKlaGGMiwF3A8bhPiw4TkW+MMUfgPqgWxn2/Q56ILDbGdANm4FatlgKXeosKGmMex334rwz3kX+A\nF4EcIATMEpF7GmbLlHLpNQClahcEvvHODh4H/uINfw64VkSOBx4CHvOGPwE8ICLH4r7zIdZEcw/g\nDu8R/zBum+4nASEROQY4CthkjNH/R9Wg9AxAKcg1xsypMmyc9/2O9/0f4AZjTFugU9wb3eYAL3vd\nR3j9iMjLUHENYImIrPWmWQm0BWYBfzHGvIrb4NiTXjvvSjUYTQBK1XINwHsfQeyo3MKt7qnadooV\nN8yh5rPqSNV5RGSdMeYg3DfDDQLmGWP6isjWem2BUvWgp5xK1e0E7/to4H9eU8urvesAACcCX3jd\nnwGnAhhjzjfG3FvbQo0xJwOni8h/RGQcsAno6McGKFUbPQNQquYqoNhLOfoYY0bhXqwd6g0bCjxk\njInivqB7lDc8D5hujLkKt65/OLW/tUmA/zPGjPOW8a6I/LQzNkapRGlroErVwhjj4F6orVqFo1Sz\noFVASimVovQMQCmlUpSeASilVIrSBKCUUilKE4BSSqUoTQBKKZWiNAEopVSK+n8fkCxeS6ZHZwAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe5632e8588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "I_ZEbLqFAL8U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Weight Regularization & Dropout"
      ]
    },
    {
      "metadata": {
        "id": "uqpguOtXAU8s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
        "model.add(Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "model.add(Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
        "\n",
        "model.compile(optimizer='rmsprop',  # Good default optimizer to start with\n",
        "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "              metrics=['accuracy'])  # what to track"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IcMLs7J7BIzu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
